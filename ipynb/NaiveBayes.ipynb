{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/buming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/buming/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/buming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenization(sentence):\n",
    "    token_method = TweetTokenizer()\n",
    "    token_list = token_method.tokenize(sentence)\n",
    "    return token_list\n",
    "\n",
    "def Cleaner(token_list, stop_words=(), english_punctuations=()):\n",
    "    #pos_tag && Lemmatisation\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(token_list):\n",
    "        word = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\"\",word)\n",
    "        word = re.sub(r\"@[a-zA-Z0-9_]+\",\"\",word)\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(wordnet.lemmatize(word.lower(),pos=pos))\n",
    "\n",
    "    #remove stop_words and the punctuations    \n",
    "    cleaned_token_list = []\n",
    "    for word in lemmatized_sentence:\n",
    "        if word.lower() not in stop_words and len(word) > 0 and word not in english_punctuations:\n",
    "            cleaned_token_list.append(word)\n",
    "    return cleaned_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "english_punctuations = []\n",
    "punc = string.punctuation\n",
    "for pun in punc:\n",
    "    english_punctuations.append(pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "for positive_sentence in positive_tweets:\n",
    "    positive_cleaned_tokens_list.append(Cleaner(Tokenization(positive_sentence), stop_words, english_punctuations))\n",
    "\n",
    "for negative_sentence in negative_tweets:\n",
    "    negative_cleaned_tokens_list.append(Cleaner(Tokenization(negative_sentence), stop_words, english_punctuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model = []\n",
    "for pos_sen in positive_cleaned_tokens_list:\n",
    "    pos_model.append(dict([word, True] for word in pos_sen))\n",
    "pos_dataset = [(pos_dict,\"Positive\") for pos_dict in pos_model]\n",
    "\n",
    "neg_model = []\n",
    "for neg_sen in negative_cleaned_tokens_list:\n",
    "    neg_model.append(dict([word, True] for word in neg_sen))\n",
    "neg_dataset = [(neg_dict,\"Negative\") for neg_dict in neg_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pos_dataset + neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset[0:7500]; test_set = dataset[7500:10000] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = {}\n",
    "all_neg_words = {}\n",
    "\n",
    "for temp in train_set:\n",
    "    if temp[1] == \"Positive\":\n",
    "        for word in temp[0]:\n",
    "            if word in all_pos_words.keys():\n",
    "                all_pos_words[word] = all_pos_words[word] + 1\n",
    "            else:\n",
    "                all_pos_words.setdefault(word, 1)\n",
    "    elif temp[1] == 'Negative':\n",
    "        for word in temp[0]:\n",
    "            if word in all_neg_words.keys():\n",
    "                all_neg_words[word] = all_neg_words[word] + 1\n",
    "            else:\n",
    "                all_neg_words.setdefault(word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = list(all_pos_words.keys())\n",
    "neg_words = list(all_neg_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence, stop_words, english_punctuations, pos_words, neg_words):\n",
    "#     cleaned_sentence = Cleaner(Tokenization(sentence), stop_words, english_punctuations)\n",
    "    pos_pro_ln = 0.0 ; neg_pro_ln = 0.0\n",
    "    for word in sentence:\n",
    "        pos_count = 1.0 ; neg_count = 1.0\n",
    "        pos_denom = 2.0 ; neg_denom = 2.0\n",
    "        if word in pos_words:\n",
    "            pos_count += 1.0 ; pos_denom += 1.0\n",
    "        if word in neg_words:\n",
    "            neg_count += 1.0 ; neg_denom += 1.0\n",
    "        pos_pro_ln = pos_pro_ln + math.log(pos_count/pos_denom) + math.log(1.0/len(pos_words))\n",
    "        neg_pro_ln = neg_pro_ln + math.log(neg_count/neg_denom) + math.log(1.0/len(neg_words))\n",
    "#     print(pos_pro_ln, neg_pro_ln)\n",
    "    pos_pro = math.exp(pos_pro_ln)\n",
    "    neg_pro = math.exp(neg_pro_ln)\n",
    "#     print('pos_pro: ', pos_pro)\n",
    "#     print('neg_pro: ', neg_pro)\n",
    "    if pos_pro > neg_pro:\n",
    "#         print('Positive')\n",
    "        return pos_pro, 'Positive'\n",
    "    else:\n",
    "#         print('Negative')\n",
    "        return neg_pro, 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eva(test_set, stop_words, english_punctuations, pos_words, neg_words):\n",
    "    tp, fp, fn, tn = 0, 0, 0, 0\n",
    "    for i in range(len(test_set)):\n",
    "        score, pred = predict(test_set[i][0], stop_words, english_punctuations, pos_words, neg_words)\n",
    "        if pred == 'Positive' and test_set[i][1] == 'Positive':\n",
    "            tp += 1\n",
    "        elif pred == 'Positive' and test_set[i][1] == 'Negative':\n",
    "            fn += 1\n",
    "        elif pred == 'Negative' and test_set[i][1] == 'Positive':\n",
    "            fp += 1\n",
    "        elif pred == 'Negative' and test_set[i][1] == 'Negative':\n",
    "            tn += 1\n",
    "    print(tp, fp, fn, tn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    f1 = 2 * precision * recall /(precision + recall)\n",
    "    accuracy = (tp+tn)/len(test_set)\n",
    "    return precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339 890 59 1212\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1, accuracy = get_eva(test_set, stop_words, english_punctuations, pos_words, neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2758340113913751, 0.8517587939698492, 0.4167178856791641, 0.6204)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision, recall, f1, accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
